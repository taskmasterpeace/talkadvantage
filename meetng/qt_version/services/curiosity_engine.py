from enum import Enum
from dataclasses import dataclass
from datetime import datetime
from typing import List, Dict, Optional, Any
import json

class QuestionType(Enum):
    YES_NO = "yes_no"
    MULTIPLE_CHOICE = "multiple_choice" 
    MULTIPLE_CHOICE_FILL = "multiple_choice_fill"
    SPEAKER_IDENTIFICATION = "speaker_identification"  # New type for speaker identification
    MEETING_TYPE = "meeting_type"  # New type for meeting type identification

# Emoji mapping for question types
QUESTION_TYPE_EMOJIS = {
    QuestionType.YES_NO: "âœ…",  # Checkmark for yes/no
    QuestionType.MULTIPLE_CHOICE: "ðŸ”¢",  # Numbers for multiple choice
    QuestionType.MULTIPLE_CHOICE_FILL: "âœï¸",  # Pencil for fill-in
    QuestionType.SPEAKER_IDENTIFICATION: "ðŸ—£ï¸",  # Speaking face for speaker ID
    QuestionType.MEETING_TYPE: "ðŸ”",  # Magnifying glass for meeting type
}

@dataclass
class CuriosityQuestion:
    """A question generated by the Curiosity Engine"""
    type: QuestionType
    text: str
    context: str
    purpose: str
    choices: List[str] = None
    timestamp: datetime = None
    answer: Any = None
    quoted_text: str = None  # Added field for speaker identification questions
    
    def to_dict(self) -> dict:
        """Convert to dictionary for storage"""
        return {
            "type": self.type.value,
            "text": self.text,
            "context": self.context,
            "purpose": self.purpose,
            "choices": self.choices,
            "timestamp": self.timestamp.isoformat() if self.timestamp else None,
            "answer": self.answer,
            "quoted_text": self.quoted_text  # Include in dictionary
        }
    
    @classmethod
    def from_dict(cls, data: dict) -> 'CuriosityQuestion':
        """Create from dictionary"""
        return cls(
            type=QuestionType(data["type"]),
            text=data["text"],
            context=data["context"],
            purpose=data["purpose"],
            choices=data.get("choices"),
            timestamp=datetime.fromisoformat(data["timestamp"]) if data.get("timestamp") else None,
            answer=data.get("answer"),
            quoted_text=data.get("quoted_text")  # Extract from dictionary
        )

class CuriosityEngine:
    """Engine for generating contextual questions and gathering insights"""
    
    def __init__(self):
        self.question_bank = self._initialize_question_bank()
        self.asked_questions = []
        self.insights = {}
        
    def _initialize_question_bank(self) -> Dict[str, List[CuriosityQuestion]]:
        """Initialize the question bank with categorized questions"""
        return {
            'user_identity': [
                CuriosityQuestion(
                    type=QuestionType.YES_NO,
                    text="Are you the primary decision maker in this discussion?",
                    context="During decision-making moments",
                    purpose="Understand user's role and authority"
                ),
                CuriosityQuestion(
                    type=QuestionType.MULTIPLE_CHOICE,
                    text="What best describes your position?",
                    context="Initial meeting analysis",
                    purpose="Identify user's perspective",
                    choices=["Leader", "Participant", "Observer", "Facilitator"]
                )
            ],
            'user_goals': [
                CuriosityQuestion(
                    type=QuestionType.MULTIPLE_CHOICE_FILL,
                    text="What's your primary goal for this interaction?",
                    context="Start of new discussion",
                    purpose="Understand user's main objective",
                    choices=[
                        "Make decisions",
                        "Share information",
                        "Build consensus",
                        "Solve problems"
                    ]
                ),
                CuriosityQuestion(
                    type=QuestionType.YES_NO,
                    text="Is this part of a larger ongoing initiative?",
                    context="Project discussions",
                    purpose="Understand broader context"
                )
            ],
            'context_understanding': [
                CuriosityQuestion(
                    type=QuestionType.MULTIPLE_CHOICE,
                    text="How would you characterize the current discussion?",
                    context="During active discussion",
                    purpose="Understand discussion dynamics",
                    choices=[
                        "Strategic planning",
                        "Problem solving",
                        "Status update",
                        "Decision making"
                    ]
                ),
                CuriosityQuestion(
                    type=QuestionType.MULTIPLE_CHOICE_FILL,
                    text="What's the most critical factor influencing this topic?",
                    context="Complex discussions",
                    purpose="Identify key drivers",
                    choices=[
                        "Time constraints",
                        "Resource limitations",
                        "Stakeholder needs",
                        "Technical challenges"
                    ]
                )
            ]
        }
        
    def _generate_regular_questions(self, transcript: str, template: dict) -> List[CuriosityQuestion]:
        """Generate regular curiosity questions based on current context using LangChain"""
        try:
            # Skip if transcript is empty
            if not transcript.strip():
                print("Empty transcript, returning no questions")
                return []
                
            print(f"Generating regular questions for transcript of length {len(transcript)}")
            print(f"Transcript preview: {transcript[:200]}...")
                
            # Define the output schema outside the method to avoid Pydantic registration issues
            from pydantic import BaseModel, Field
            from typing import List as TypeList, Optional
            
            class QuestionOutput(BaseModel):
                """Schema for LLM-generated questions"""
                question: str = Field(description="The question text")
                type: str = Field(description="Question type: YES_NO, MULTIPLE_CHOICE, or MULTIPLE_CHOICE_FILL")
                purpose: str = Field(description="The purpose of asking this question")
                choices: Optional[TypeList[str]] = Field(
                    default=None, 
                    description="For multiple choice questions, the available options (3-4 options)"
                )
                
            class QuestionsResponse(BaseModel):
                """Container for multiple questions"""
                questions: TypeList[QuestionOutput] = Field(
                    description="List of 2-3 questions about the transcript"
                )
            
            # Get model from settings
            from qt_version.utils.settings_manager import SettingsManager
            settings = SettingsManager()
            model_name = settings.get_setting('llm_model', 'gpt-4o-mini')
            
            # Get the prompt - FIRST CHECK TEMPLATE, then fallback to settings
            system_prompt = ""
            if template and "curiosity_prompt" in template and template["curiosity_prompt"]:
                # Use template-specific prompt if available
                system_prompt = template["curiosity_prompt"]
                print("Using template-specific curiosity prompt")
            else:
                # Fallback to global settings
                system_prompt = settings.get_setting('curiosity_engine_prompt', 
                    '''[SYSTEM INSTRUCTIONS - DO NOT MODIFY THIS SECTION]
You are an expert active listener analyzing meeting transcripts. 
Generate 2-3 insightful questions that would help understand the context better.

[QUESTION TYPES - DO NOT MODIFY THESE TYPES]
Question types:
- YES_NO: Simple yes/no questions
- MULTIPLE_CHOICE: Questions with predefined options (provide 3-4 choices)
- MULTIPLE_CHOICE_FILL: Multiple choice with an "other" option (provide 3-4 choices)

[CUSTOMIZABLE GUIDELINES - YOU CAN MODIFY THIS SECTION]
Generate questions that:
- Are relevant to the transcript content
- Help clarify important points
- Uncover underlying context
- Are concise and clear
- Have meaningful multiple choice options when applicable

[OUTPUT FORMAT - DO NOT MODIFY THIS SECTION]
Return a list of questions in the specified format.''')
                print("Using global curiosity prompt from settings")
            
            # Clean up the prompt by removing section headers if needed
            clean_prompt = system_prompt.replace('[SYSTEM INSTRUCTIONS - DO NOT MODIFY THIS SECTION]', '')
            clean_prompt = clean_prompt.replace('[QUESTION TYPES - DO NOT MODIFY THESE TYPES]', '')
            clean_prompt = clean_prompt.replace('[CUSTOMIZABLE GUIDELINES - YOU CAN MODIFY THIS SECTION]', '')
            clean_prompt = clean_prompt.replace('[OUTPUT FORMAT - DO NOT MODIFY THIS SECTION]', '')
            
            # Use the cleaned prompt
            system_prompt = clean_prompt
            
            # Use updated LangChain imports
            from langchain_openai import ChatOpenAI
            from langchain.prompts import ChatPromptTemplate
            
            # Create the chat model with lower temperature for structured output
            llm = ChatOpenAI(temperature=0.3, model=model_name)
            
            # Create a structured output model
            structured_llm = llm.with_structured_output(QuestionsResponse)
            
            # Create the prompt template
            prompt = ChatPromptTemplate.from_messages([
                ("system", system_prompt),
                ("human", "Based on this transcript, generate questions to better understand the context:\n\n{transcript}")
            ])
            
            # Create the chain using LCEL (LangChain Expression Language)
            chain = prompt | structured_llm
            
            # Invoke the chain with a timeout
            try:
                response = chain.invoke({"transcript": transcript}, config={"timeout": 15})
            except Exception as e:
                print(f"Error invoking LLM chain: {str(e)}")
                return []
            
            # After getting response, log it
            print(f"Generated {len(response.questions)} questions from LLM")
            
            # Convert the structured output to CuriosityQuestion objects
            questions = []
            for q_output in response.questions:
                try:
                    # Convert the question type string to QuestionType enum
                    q_type = QuestionType(q_output.type.lower())
                    print(f"Question type: {q_type}")
                except ValueError:
                    # Default to YES_NO if type is invalid
                    q_type = QuestionType.YES_NO
                    print(f"Invalid question type: {q_output.type}, defaulting to YES_NO")
                    
                # Create CuriosityQuestion
                question = CuriosityQuestion(
                    type=q_type,
                    text=q_output.question,
                    context=transcript[:100] + "...",  # Use first 100 chars of transcript as context
                    purpose=q_output.purpose,
                    choices=q_output.choices if q_type != QuestionType.YES_NO else None
                )
                
                questions.append(question)
            
            print(f"Returning {len(questions)} LLM-generated regular questions")
            return questions[:2]  # Limit to 2 regular questions

        except Exception as e:
            print(f"Error generating regular questions: {str(e)}")
            import traceback
            traceback.print_exc()
            return []
            
    def generate_questions(self, transcript: str, template: dict) -> List[CuriosityQuestion]:
        """Generate relevant questions based on current context using LangChain"""
        try:
            # Skip if transcript is empty
            if not transcript.strip():
                print("Empty transcript, returning no questions")
                return []
                
            word_count = len(transcript.split())
            print(f"Generating questions for transcript with {word_count} words and template {template.get('name', 'unknown')}")
            
            # Generate regular curiosity questions
            regular_questions = self._generate_regular_questions(transcript, template)
            
            # Generate speaker identification questions if transcript is long enough
            speaker_questions = []
            if word_count > 100:
                print(f"Transcript has {word_count} words, generating speaker questions")
                speaker_questions = self._generate_speaker_questions(transcript)
                print(f"Generated {len(speaker_questions)} speaker questions")
            else:
                print(f"Transcript has only {word_count} words, skipping speaker questions")
                
            # Generate meeting type questions if transcript is long enough
            meeting_type_questions = []
            if word_count > 100:
                print(f"Transcript has {word_count} words, generating meeting type questions")
                meeting_type_questions = self._generate_meeting_type_questions(transcript)
                print(f"Generated {len(meeting_type_questions)} meeting type questions")
            else:
                print(f"Transcript has only {word_count} words, skipping meeting type questions")
                
            # Generate tree navigation questions if visualization is enabled
            tree_questions = self._generate_tree_navigation_questions(transcript)
                
            # Combine all questions
            all_questions = regular_questions + speaker_questions + meeting_type_questions + tree_questions
            
            if not all_questions:
                print("No questions generated, falling back to default questions")
                all_questions = self._get_fallback_questions(transcript, template)
                
            # Ensure all questions have proper choices if needed
            for question in all_questions:
                if question.type in [QuestionType.MULTIPLE_CHOICE, QuestionType.MULTIPLE_CHOICE_FILL] and (not question.choices or len(question.choices) < 2):
                    print(f"Warning: Question of type {question.type.value} has no choices: {question.text}")
                    question.choices = ["Option 1", "Option 2", "Option 3"]
                elif question.type == QuestionType.SPEAKER_IDENTIFICATION and (not question.choices or len(question.choices) < 2):
                    print(f"Warning: Speaker identification question has no choices: {question.text}")
                    question.choices = ["You (the user)", "Another participant", "Multiple people", "Not sure"]
                elif question.type == QuestionType.MEETING_TYPE and (not question.choices or len(question.choices) < 2):
                    print(f"Warning: Meeting type question has no choices: {question.text}")
                    question.choices = ["Discussion", "Presentation", "Interview", "Brainstorming", "Status update"]
                
            print(f"Returning {len(all_questions)} questions ({len(regular_questions)} regular, {len(speaker_questions)} speaker, {len(meeting_type_questions)} meeting type)")
            for i, q in enumerate(all_questions):
                print(f"Question {i+1}: Type={q.type.value}, Text={q.text}")
                if q.type == QuestionType.SPEAKER_IDENTIFICATION:
                    print(f"  Quote: '{q.quoted_text}'")
                print(f"  Choices: {q.choices}")
                
            # Make sure we have at least one question
            if not all_questions:
                print("Still no questions after fallback, creating a basic question")
                all_questions = [
                    CuriosityQuestion(
                        type=QuestionType.YES_NO,
                        text="Is this conversation meeting your expectations?",
                        context="General feedback",
                        purpose="Understand user satisfaction"
                    )
                ]
                
            return all_questions[:5]  # Limit to 5 questions total

        except Exception as e:
            print(f"Error generating questions: {str(e)}")
            import traceback
            traceback.print_exc()
            # Fall back to predefined questions if LangChain fails
            return self._get_fallback_questions(transcript, template)
        
    def process_answer(self, question: CuriosityQuestion, answer: Any):
        """Process and store user's answer"""
        question.answer = answer
        question.timestamp = datetime.now()
        self.asked_questions.append(question)
        
        # Update insights based on answer
        self._update_insights(question)
        
    def _update_insights(self, question: CuriosityQuestion):
        """Update insights based on answered question"""
        category = next(
            (cat for cat, questions in self.question_bank.items() 
             if any(q.text == question.text for q in questions)),
            "general"
        )
        
        if category not in self.insights:
            self.insights[category] = []
            
        insight = {
            "question": question.text,
            "answer": question.answer,
            "purpose": question.purpose,
            "timestamp": question.timestamp
        }
        self.insights[category].append(insight)
        
    def get_insights(self) -> Dict[str, List[dict]]:
        """Get accumulated insights from answered questions"""
        return self.insights
        
    def save_state(self, filepath: str):
        """Save engine state to file"""
        state = {
            "asked_questions": [q.to_dict() for q in self.asked_questions],
            "insights": self.insights
        }
        with open(filepath, 'w') as f:
            json.dump(state, f, indent=2)
            
    def load_state(self, filepath: str):
        """Load engine state from file"""
        with open(filepath, 'r') as f:
            state = json.load(f)
            self.asked_questions = [
                CuriosityQuestion.from_dict(q) for q in state["asked_questions"]
            ]
            self.insights = state["insights"]
            
    def set_template(self, template: dict):
        """Set the template for the curiosity engine
        
        Args:
            template: The template dictionary
        """
        self.current_template = template
        print(f"Curiosity Engine: Template set to '{template.get('name', 'unknown')}'")
            
    def _extract_potential_quotes(self, transcript: str) -> List[str]:
        """Extract potential quotes from the transcript to ask about"""
        import re
        import random
        
        # Split transcript into sentences
        sentences = re.split(r'[.!?]+', transcript)
        
        # Filter with looser criteria
        potential_quotes = [
            s.strip() for s in sentences 
            if len(s.strip().split()) >= 3  # At least 3 words
            and len(s.strip()) > 10  # At least 10 characters
        ]
        
        # If no quotes found, use any non-empty sentences
        if not potential_quotes and sentences:
            potential_quotes = [s.strip() for s in sentences if s.strip()]
            
        # Log what we found
        print(f"Found {len(potential_quotes)} potential quotes from {len(sentences)} sentences")
        if potential_quotes:
            print(f"Sample quote: '{potential_quotes[0]}'")
        
        # Shuffle and take up to 5 potential quotes
        random.shuffle(potential_quotes)
        return potential_quotes[:5]
        
    def _generate_meeting_type_questions(self, transcript: str) -> List[CuriosityQuestion]:
        """Generate questions to identify the type of meeting/conversation"""
        try:
            # Skip if transcript is too short
            word_count = len(transcript.split())
            if word_count < 100:
                print(f"Transcript too short for meeting type identification ({word_count} words < 100)")
                return []
                
            print(f"Generating meeting type questions for transcript with {word_count} words")
            
            # Get model and prompt from settings
            from qt_version.utils.settings_manager import SettingsManager
            settings = SettingsManager()
            model_name = settings.get_setting('llm_model', 'gpt-4o-mini')
            
            # Define the output schema
            from pydantic import BaseModel, Field
            from typing import List as TypeList
            
            class MeetingTypeQuestionOutput(BaseModel):
                """Schema for meeting type identification question"""
                question: str = Field(description="The question text asking about the meeting type")
                purpose: str = Field(description="The purpose of asking this question")
                choices: TypeList[str] = Field(
                    description="Multiple choice options for meeting types (4-5 options)"
                )
            
            # Use updated LangChain imports
            from langchain_openai import ChatOpenAI
            from langchain.prompts import ChatPromptTemplate
            
            # Create the chat model
            llm = ChatOpenAI(temperature=0.3, model=model_name)
            
            # Create a structured output model
            structured_llm = llm.with_structured_output(MeetingTypeQuestionOutput)
            
            # Create the prompt template
            prompt = ChatPromptTemplate.from_messages([
                ("system", """You are an expert at analyzing conversation transcripts.
                
                Based on the provided transcript, generate a question to identify what type of meeting or conversation this is.
                
                The question should be direct and focused on determining the meeting type.
                
                Include multiple choice options that cover the most likely meeting types based on the transcript content.
                
                Common meeting types include: presentation, discussion, negotiation, interview, brainstorming, status update, training, etc.
                
                Make your best guess about what type of meeting this is, and include that as one of the options."""),
                ("human", f"Based on this transcript, generate a question to identify the meeting type:\n\n{transcript[:1000]}")
            ])
            
            # Create the chain
            chain = prompt | structured_llm
            
            # Invoke the chain with a timeout
            try:
                response = chain.invoke({}, config={"timeout": 10})
                
                # Create question object
                question = CuriosityQuestion(
                    type=QuestionType.MEETING_TYPE,
                    text=response.question,
                    context="Based on the conversation so far",
                    purpose=response.purpose,
                    choices=response.choices
                )
                
                # Ensure we have choices
                if not question.choices or len(question.choices) < 2:
                    # Provide fallback choices if none were generated
                    question.choices = ["Discussion", "Presentation", "Interview", "Brainstorming", "Status update"]
                    print(f"Using fallback choices for meeting type: {question.choices}")
                
                print(f"Created meeting type question: {question.text}")
                print(f"With choices: {question.choices}")
                
                return [question]
                
            except Exception as e:
                print(f"Error generating meeting type question: {str(e)}")
                import traceback
                traceback.print_exc()
                return []
                
        except Exception as e:
            print(f"Error in _generate_meeting_type_questions: {str(e)}")
            import traceback
            traceback.print_exc()
            return []
            
    def _generate_speaker_questions(self, transcript: str) -> List[CuriosityQuestion]:
        """Generate questions to identify speakers in the transcript"""
        try:
            # Skip if transcript is too short
            word_count = len(transcript.split())
            if word_count < 100:
                print(f"Transcript too short for speaker identification ({word_count} words < 100)")
                return []
                
            # Extract potential quotes to ask about
            quotes = self._extract_potential_quotes(transcript)
            if not quotes:
                print("No potential quotes found for speaker identification")
                return []
                
            # Select up to 2 quotes to ask about
            selected_quotes = quotes[:2]
            print(f"Selected {len(selected_quotes)} quotes for speaker identification questions")
            questions = []
            
            # Get model and prompt from settings
            from qt_version.utils.settings_manager import SettingsManager
            settings = SettingsManager()
            model_name = settings.get_setting('llm_model', 'gpt-4o-mini')
            
            # Get the speaker identification prompt
            speaker_prompt = settings.get_setting('speaker_identification_prompt', 
                '''You are a conversation analysis expert specializing in speaker identification.
                
                Based on the following transcript excerpt, generate a question to confirm who said the quoted text.
                The question should help identify the speaker of this statement.
                
                Make the question concise and direct, focusing specifically on who said the quoted text.
                
                IMPORTANT: For the multiple choice options, use descriptive labels like "You (the user)", 
                "The main presenter", "A meeting participant", etc. rather than generic labels like "Speaker A".
                
                The question should be phrased in a way that can be answered with a multiple choice selection.''')
            
            # Use updated LangChain imports
            from langchain_openai import ChatOpenAI
            from langchain.prompts import ChatPromptTemplate
            
            # Define the output schema
            from pydantic import BaseModel, Field
            from typing import List as TypeList
            
            class SpeakerQuestionOutput(BaseModel):
                """Schema for speaker identification question"""
                question: str = Field(description="The question text asking who said the quoted statement")
                purpose: str = Field(description="The purpose of asking this question (e.g., 'Identify the speaker')")
                choices: TypeList[str] = Field(
                    description="Multiple choice options for who might have said the quote. Use descriptive labels like 'You (the user)', 'The main presenter', 'A meeting participant', etc. rather than generic labels like 'Speaker A'."
                )
            
            # Create the chat model
            llm = ChatOpenAI(temperature=0.3, model=model_name)
            
            # Create a structured output model
            structured_llm = llm.with_structured_output(SpeakerQuestionOutput)
            
            for quote in selected_quotes:
                # Create the prompt template
                prompt = ChatPromptTemplate.from_messages([
                    ("system", speaker_prompt),
                    ("human", f"Based on this transcript, generate a question to identify who said: \"{quote}\"\n\nTranscript excerpt:\n{transcript[:1000]}")
                ])
                
                # Create the chain
                chain = prompt | structured_llm
                
                # Invoke the chain with a timeout
                try:
                    response = chain.invoke({}, config={"timeout": 10})
                    
                    # Create question object
                    question = CuriosityQuestion(
                        type=QuestionType.SPEAKER_IDENTIFICATION,
                        text=response.question,
                        context=f"This relates to the statement: '{quote}'",
                        purpose=response.purpose,
                        choices=response.choices,
                        quoted_text=quote
                    )
                    
                    # Ensure we have choices
                    if not question.choices or len(question.choices) < 2:
                        # Provide fallback choices if none were generated
                        question.choices = ["You (the user)", "Another participant", "Multiple people", "Not sure"]
                        print(f"Using fallback choices for speaker identification: {question.choices}")
                    questions.append(question)
                    print(f"Created speaker identification question: {question.text}")
                    print(f"With choices: {question.choices}")
                    
                except Exception as e:
                    print(f"Error generating speaker question: {str(e)}")
                    import traceback
                    traceback.print_exc()
                    
            return questions
            
        except Exception as e:
            print(f"Error in _generate_speaker_questions: {str(e)}")
            import traceback
            traceback.print_exc()
            return []
    
    def _get_fallback_questions(self, transcript: str, template: dict) -> List[CuriosityQuestion]:
        """Provide fallback questions when LLM generation fails"""
        # Select a few generic questions from the question bank
        fallback_questions = []
        
        # Add one from each category to ensure variety
        for category in self.question_bank.values():
            if category and len(fallback_questions) < 3:
                # Get a question that hasn't been asked yet
                for question in category:
                    if question.text not in {q.text for q in self.asked_questions}:
                        # Make a copy of the question to avoid modifying the original
                        fallback_question = CuriosityQuestion(
                            type=question.type,
                            text=question.text,
                            context=question.context,
                            purpose=question.purpose,
                            choices=question.choices.copy() if question.choices else None
                        )
                        fallback_questions.append(fallback_question)
                        break
        
        # If we still need more questions, add generic ones
        if len(fallback_questions) < 2:
            generic_question = CuriosityQuestion(
                type=QuestionType.YES_NO,
                text="Is this discussion meeting your expectations?",
                context="General feedback",
                purpose="Understand user satisfaction"
            )
            fallback_questions.append(generic_question)
            
        # Add a speaker identification question if transcript is long enough
        word_count = len(transcript.split())
        if word_count > 100 and len(fallback_questions) < 3:
            speaker_question = CuriosityQuestion(
                type=QuestionType.SPEAKER_IDENTIFICATION,
                text="Who do you think made the most important points in this conversation?",
                context="Speaker analysis",
                purpose="Identify key contributors",
                choices=["You (the user)", "Another participant", "Multiple people", "Not sure"]
            )
            fallback_questions.append(speaker_question)
            
        # Add a meeting type question if transcript is long enough
        if word_count > 100 and len(fallback_questions) < 3:
            meeting_question = CuriosityQuestion(
                type=QuestionType.MEETING_TYPE,
                text="What type of conversation would you categorize this as?",
                context="Meeting analysis",
                purpose="Identify conversation type",
                choices=["Discussion", "Presentation", "Interview", "Brainstorming", "Status update"]
            )
            fallback_questions.append(meeting_question)
            
        # Ensure all questions have proper choices if needed
        for question in fallback_questions:
            if question.type in [QuestionType.MULTIPLE_CHOICE, QuestionType.MULTIPLE_CHOICE_FILL] and not question.choices:
                question.choices = ["Option 1", "Option 2", "Option 3"]
            elif question.type == QuestionType.SPEAKER_IDENTIFICATION and not question.choices:
                question.choices = ["You (the user)", "Another participant", "Multiple people", "Not sure"]
            elif question.type == QuestionType.MEETING_TYPE and not question.choices:
                question.choices = ["Discussion", "Presentation", "Interview", "Brainstorming", "Status update"]
                
        return fallback_questions[:3]  # Limit to 3 questions
    def _generate_tree_navigation_questions(self, transcript: str) -> List[CuriosityQuestion]:
        """Generate questions to help with tree navigation"""
        # Create a question about which part of the conversation to focus on
        focus_question = CuriosityQuestion(
            type=QuestionType.MULTIPLE_CHOICE,
            text="Which part of the conversation would you like to explore in more detail?",
            context="Tree navigation",
            purpose="Focus the conversation tree on relevant areas",
            choices=[
                "Current topic",
                "Previous topics",
                "Decision points",
                "Action items",
                "Show entire conversation"
            ]
        )
        
        # Create a question about tree layout preference
        layout_question = CuriosityQuestion(
            type=QuestionType.MULTIPLE_CHOICE,
            text="How would you prefer to view the conversation tree?",
            context="Visualization preference",
            purpose="Adjust tree layout to user preference",
            choices=[
                "Radial layout (branches out in all directions)",
                "Hierarchical layout (top to bottom)",
                "Compact view (focus on current branch)",
                "Expanded view (show all branches)"
            ]
        )
        
        return [focus_question, layout_question]
